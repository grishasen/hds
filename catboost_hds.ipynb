{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tempfile\n",
    "import os\n",
    "import zipfile\n",
    "import shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset_export(file_name, src_folder=\".\",\n",
    "                        tmp_folder=None,\n",
    "                        lazy=False,\n",
    "                        verbose=False):\n",
    "    json_file = None\n",
    "    error_reason = \"\"\n",
    "    tmp_folder = tmp_folder if tmp_folder else tempfile.gettempdir()\n",
    "\n",
    "    if file_name.endswith(\".json\"):\n",
    "        error_reason = \"Error reading JSON file\"\n",
    "        if os.path.exists(file_name):\n",
    "            json_file = file_name\n",
    "        elif os.path.exists(os.path.join(src_folder, file_name)):\n",
    "            json_file = os.path.join(src_folder, file_name)\n",
    "        if json_file and verbose:\n",
    "            print(error_reason, json_file)\n",
    "        if json_file:\n",
    "            if lazy:\n",
    "                multi_line_json = pl.scan_ndjson(json_file)\n",
    "            else:\n",
    "                multi_line_json = pl.read_ndjson(json_file)\n",
    "\n",
    "    else:\n",
    "        zip_file = file_name\n",
    "        if file_name.endswith(\".zip\"):\n",
    "            error_reason = \"Error reading ZIP file\"\n",
    "            if os.path.exists(file_name):\n",
    "                zip_file = file_name\n",
    "            elif os.path.exists(os.path.join(src_folder, file_name)):\n",
    "                zip_file = os.path.join(src_folder, file_name)\n",
    "            if verbose:\n",
    "                print(error_reason, zip_file)\n",
    "\n",
    "            if os.path.exists(zip_file):\n",
    "                error_reason = \"Error extracting data.json\"\n",
    "                if verbose:\n",
    "                    print(error_reason, zip_file)\n",
    "\n",
    "                json_file = os.path.join(tmp_folder, \"data.json\")\n",
    "                if os.path.exists(json_file):\n",
    "                    os.remove(json_file)\n",
    "\n",
    "                with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "                    all_zip_entries = zip_ref.namelist()\n",
    "                    json_file_in_zip = [s for s in all_zip_entries if \"data.json\" in s]\n",
    "                    if verbose:\n",
    "                        print(\"data.json in zip file:\", json_file_in_zip, zip_file)\n",
    "\n",
    "                    for file in json_file_in_zip:\n",
    "                        zip_ref.extract(file, tmp_folder)\n",
    "                        json_file = os.path.join(tmp_folder, file)\n",
    "\n",
    "                if not os.path.exists(json_file):\n",
    "                    raise Exception(f\"Dataset zipfile {zip_file} does not have \\\"data.json\\\"\")\n",
    "                if lazy:\n",
    "                    multi_line_json = pl.scan_ndjson(json_file, infer_schema_length=100000)\n",
    "                else:\n",
    "                    multi_line_json = pl.read_ndjson(json_file, infer_schema_length=100000)\n",
    "                    os.remove(json_file)\n",
    "\n",
    "    if json_file is None:\n",
    "        raise Exception(f\"Dataset export not found {error_reason}\")\n",
    "    return multi_line_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_dataset_export( \"Web_ClickThrough.zip\", lazy=True, verbose=True)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.collect_schema().names()\n",
    "columns.sort()\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.unique(subset=['Decision_InteractionID', 'Context_Treatment'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns(\n",
    "    pl.when(pl.col(pl.String).str.len_chars() == 0)\n",
    "    .then(None)\n",
    "    .otherwise(pl.col(pl.String))\n",
    "    .name.keep()\n",
    "    ).with_columns(\n",
    "        cs.ends_with(\"_DaysSince\", \n",
    "                     \"_pyHistoricalOutcomeCount\",\n",
    "                     \"DaysinCurrentStage\")\n",
    "                     .cast(pl.Float64).fill_null(0),\n",
    "        pl.col(\n",
    "            [\n",
    "                \"Customer_AnnualIncome\",\n",
    "                \"Customer_CreditScore\",\n",
    "                \"Customer_DebtToIncomeRatio\",\n",
    "                \"Customer_NetWealth\",\n",
    "                \"Customer_RelationshipLengthDays\",\n",
    "                \"Customer_TotalAssets\",\n",
    "                \"Customer_TotalLiabilities\",\n",
    "                \"Customer_BirthDate\"\n",
    "            ]\n",
    "            )\n",
    "        .cast(pl.Float64)\n",
    "        .fill_null(0),\n",
    "        cs.starts_with(\"Customer_Num\").cast(pl.Float64).fill_null(0),\n",
    "        cs.starts_with(\"Context_\").cast(pl.String),\n",
    "        cs.starts_with(\"Customer_Is\").replace_strict({\"false\":False, \"true\":True, \"null\":False, \"False\":False, \"True\":True}),\n",
    "        cs.starts_with(\"Customer_Has\").replace_strict({\"false\":False, \"true\":True, \"null\":False, \"False\":False, \"True\":True})\n",
    "    ).with_columns(\n",
    "        cs.starts_with(\"Customer_Is\").fill_null(False).cast(pl.Boolean),\n",
    "        cs.starts_with(\"Customer_Has\").fill_null(False).cast(pl.Boolean)\n",
    "    ).with_columns(\n",
    "        pl.col(\n",
    "            [\n",
    "                \"Customer_AnnualIncome\",\n",
    "                \"Customer_CreditScore\",\n",
    "                \"Customer_DebtToIncomeRatio\",\n",
    "                \"Customer_NetWealth\",\n",
    "                \"Customer_RelationshipLengthDays\",\n",
    "                \"Customer_TotalAssets\",\n",
    "                \"Customer_TotalLiabilities\"\n",
    "            ]\n",
    "        ).cast(pl.Float64).fill_null(0),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"rulesetVersion\", \"id\", \"dataCenter\", \"negativeSampling\", \"positiveSampling\", \"rulesetName\",\n",
    "                \"Decision_SubjectID\", \"Decision_OutcomeTime\", \"Decision_Rank\", \"Decision_InteractionID\",\n",
    "                \"Decision_DecisionTime\", \"Decision_OutcomeWeight\", \"pyModelEvidence\", \"pyModelPerformance\", \n",
    "                \"pyModelPositives\", \"pyPropensity\", \"rulesetVersion\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = list()\n",
    "schema = df.collect_schema()\n",
    "\n",
    "for cname in schema.names():\n",
    "    ctype = schema[cname]\n",
    "    if(not(cname.startswith(\"Decision_\")) and pl.String.is_(ctype)):\n",
    "        df = df.with_columns(pl.col(cname).fill_null('N/A'))\n",
    "        cat_features.append(cname)\n",
    "print(cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processing_options = {\n",
    "    \"tokenizers\": [{\n",
    "        \"tokenizer_id\": \"comma\",\n",
    "        \"delimiter\": \",\",\n",
    "        \"lowercasing\": \"true\"\n",
    "    }],\n",
    "\n",
    "    \"dictionaries\": [{\n",
    "        \"dictionary_id\": \"Word\",\n",
    "        \"gram_order\": \"1\"\n",
    "    }],\n",
    "\n",
    "    \"feature_processing\": {\n",
    "        \"default\": [{\n",
    "            \"dictionaries_names\": [\"Word\"],\n",
    "            \"feature_calcers\": [\"BoW\"],\n",
    "            \"tokenizers_names\": [\"comma\"]\n",
    "        }]\n",
    "    }\n",
    "}\n",
    "text_features = ['Customer_OwnedAccountTypes']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = list(set(cat_features) - set(text_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.collect()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = df.to_pandas()\n",
    "y = dset['Decision_Outcome']\n",
    "X = dset.drop(['Decision_Outcome'], axis=1)\n",
    "seed = 127\n",
    "test_size = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=seed)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {'loss_function': 'Logloss',  # objective function\n",
    "          'eval_metric': 'AUC',  # metric\n",
    "          'verbose': 50,  # output to stdout info about training process every 50 iterations\n",
    "          'random_seed': seed,\n",
    "          'cat_features': cat_features,\n",
    "          'text_features': text_features,\n",
    "          'text_processing': text_processing_options,\n",
    "          'one_hot_max_size': 255,\n",
    "          'class_names': ['NoResponse', 'Clicked'],\n",
    "          'iterations': 100,\n",
    "          'learning_rate': 0.5,\n",
    "          'depth': 8\n",
    "          }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cbc_1 = CatBoostClassifier(**params)\n",
    "cbc_1.fit(X=X_train, y=y_train,  # data to train on (required parameters, unless we provide X as a pool object, will be shown below)\n",
    "          eval_set=(X_val, y_val),  # data to validate on\n",
    "          # True if we don't want to save trees created after iteration with the best validation score\n",
    "          use_best_model=True,\n",
    "          # True for visualization of the training process (it is not shown in a published kernel - try executing this code)\n",
    "          plot=True\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool(X_test, y_test, cat_features=cat_features, text_features=text_features)\n",
    "#pool = Pool(X_test, y_test, cat_features=cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbc_1.get_all_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbc_1.plot_tree(\n",
    "    tree_idx=0,\n",
    "    pool=pool\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = cbc_1.get_feature_importance(data=pool,\n",
    "                                                  prettified=True,\n",
    "                                                  verbose=True, type=\"PredictionValuesChange\")\n",
    "feature_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = cbc_1.get_feature_importance(data=pool,\n",
    "                                                  prettified=True,\n",
    "                                                  verbose=True, type=\"LossFunctionChange\")\n",
    "feature_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the prediction using the resulting model\n",
    "preds = cbc_1.predict(pool)\n",
    "preds_proba = cbc_1.predict_proba(pool)\n",
    "print(preds_proba[:5])\n",
    "print(cbc_1.predict(pool, 'RawFormulaVal')[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test, preds, labels=params.get('class_names')))\n",
    "print(metrics.classification_report(\n",
    "    y_test, preds, labels=params.get('class_names')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost.utils import get_roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "curve = get_roc_curve(cbc_1, pool)\n",
    "(fpr, tpr, thresholds) = curve\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "lw = 2\n",
    "\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc, alpha=0.5)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.title('Receiver operating characteristic', fontsize=20)\n",
    "plt.legend(loc=\"lower right\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('error:', 1-np.mean(preds == np.ravel(y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_learn = pd.read_csv(\n",
    "    'catboost_info/learn_error.tsv', header=0, delimiter='\\t')\n",
    "rmse_test = pd.read_csv('catboost_info/test_error.tsv',\n",
    "                        header=0, delimiter='\\t')\n",
    "plt.plot(rmse_learn['Logloss'], label=\"Learn Error\")\n",
    "plt.plot(rmse_test['Logloss'], label=\"Test Error\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = cbc_1.get_feature_importance(pool, type=\"ShapValues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_value = shap_values[0, -1]\n",
    "shap_values = shap_values[:, :-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, max_display=20, plot_size=[14,10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\", plot_size=[14,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap.plots.force(expected_value, shap_values[50], feature_names=X_test.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'Customer_DebtToIncomeRatio'\n",
    "res = cbc_1.calc_feature_statistics(X_test, y_test, feature, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse Model Without Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = df.to_pandas()\n",
    "y = dset['Decision_Outcome']\n",
    "X = dset.drop(['Decision_Outcome'] + text_features, axis=1)\n",
    "seed = 127\n",
    "test_size = 0.2\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=seed)\n",
    "X_train2, X_val2, y_train2, y_val2 = train_test_split(\n",
    "    X_train2, y_train2, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'loss_function': 'Logloss',  # objective function\n",
    "          'eval_metric': 'AUC',  # metric\n",
    "          'verbose': 50,  # output to stdout info about training process every 50 iterations\n",
    "          'random_seed': seed,\n",
    "          'cat_features': cat_features,\n",
    "          'class_names': ['NoResponse', 'Clicked'],\n",
    "          'iterations': 100,\n",
    "          'learning_rate': 0.5,\n",
    "          'depth': 8\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cbc_2 = CatBoostClassifier(**params)\n",
    "cbc_2.fit(X=X_train2, y=y_train2,  # data to train on (required parameters, unless we provide X as a pool object, will be shown below)\n",
    "          eval_set=(X_val2, y_val2),  # data to validate on\n",
    "          # True if we don't want to save trees created after iteration with the best validation score\n",
    "          use_best_model=True,\n",
    "          # True for visualization of the training process (it is not shown in a published kernel - try executing this code)\n",
    "          plot=True\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score_diff(first_model, second_model):\n",
    "    first_accuracy = first_model.best_score_['validation']['AUC']\n",
    "    second_accuracy = second_model.best_score_['validation']['AUC']\n",
    "\n",
    "    gap = (second_accuracy - first_accuracy) / first_accuracy * 100\n",
    "\n",
    "    print('{} vs {} ({:+.2f}%)'.format(first_accuracy, second_accuracy, gap))\n",
    "print('Model AUC difference - without text features vs with text features.')\n",
    "print_score_diff(cbc_2, cbc_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(cbc_2)\n",
    "shap_values_exp = explainer(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.force(explainer(X_test2.sample(n=500, random_state=seed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"Customer_CLV\", shap_values_exp.values, X_test2, interaction_index=\"Customer_DebtToIncomeRatio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Predition Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.force(shap_values_exp[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_values_exp[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_proba = cbc_2.predict_proba(X_test2.iloc[8])\n",
    "print(preds_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.decision_plot(\n",
    "    base_value=np.array([explainer.expected_value]),\n",
    "    shap_values=explainer.shap_values(X_test2)[8],\n",
    "    features=X_test2.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'Customer_CLV'\n",
    "shap.plots.scatter(shap_values_exp[:, feature], color=shap_values_exp[:, \"Customer_CreditScore\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using global feature importance orderings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values_exp[:, shap_values_exp.abs.mean(0).argsort[-1]], alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Calibration Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calibration curves\n",
    "def calibration(groundtruth, probs):\n",
    "    # Convert groundtruth to binary and ensure probabilities are in a DataFrame\n",
    "    groundtruth_binary = 1*np.array(groundtruth)\n",
    "    nlabels = len(np.unique(groundtruth))\n",
    "    \n",
    "    if nlabels < 2:\n",
    "        return pl.DataFrame({\n",
    "            \"MeanProbs\": [0.5],\n",
    "            \"PositivesShare\": [None],\n",
    "            \"binPos\": [None],\n",
    "            \"binNeg\": [None]\n",
    "        })\n",
    "\n",
    "    if nlabels > 2:\n",
    "        raise ValueError(\"'groundtruth' has more than two levels.\")\n",
    "    \n",
    "    # Create probabilities DataFrame with binning\n",
    "    probabilities = pl.DataFrame({\n",
    "        \"groundtruth\": groundtruth_binary,\n",
    "        \"probs\": probs\n",
    "    })\n",
    "\n",
    "    # Group and summarize probabilities\n",
    "    grouped_probabilities = (probabilities\n",
    "                             .with_columns((pl.col(\"probs\") * 10).round().alias(\"bin\"))  # Binning probs to 1 decimal place\n",
    "                             .group_by(\"bin\")\n",
    "                             .agg([\n",
    "                                 pl.mean(\"probs\").alias(\"MeanProbs\"),\n",
    "                                 pl.sum(\"groundtruth\").alias(\"binPos\"),\n",
    "                                 (pl.count(\"groundtruth\") - pl.sum(\"groundtruth\")).alias(\"binNeg\"),\n",
    "                                 (pl.sum(\"groundtruth\") / pl.count(\"groundtruth\")).alias(\"PositivesShare\")\n",
    "                             ])\n",
    "                             .sort(\"bin\"))\n",
    "    return grouped_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_bin = y_test.apply(lambda x: x == 'Clicked')\n",
    "preds_proba = cbc_1.predict_proba(X_test)\n",
    "calibration_data = calibration(y_test_bin, preds_proba[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = px.line(calibration_data.to_pandas(), \n",
    "              x=\"MeanProbs\", \n",
    "              y=\"PositivesShare\")\n",
    "\n",
    "\n",
    "# Add ideal calibration line (diagonal)\n",
    "fig.add_shape(type=\"line\", line=dict(dash='dash', color=\"darkred\"), row='all', col='all', x0=0, y0=0, x1=1, y1=1)\n",
    "\n",
    "# Customize the layout and labels\n",
    "fig.update_layout(\n",
    "    title=\"Model calibration plot\",\n",
    "    xaxis_title=\"Mean predicted probability\",\n",
    "    yaxis_title=\"Fraction of positives\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
